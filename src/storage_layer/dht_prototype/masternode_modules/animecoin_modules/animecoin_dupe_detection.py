import random
import functools
import time
import multiprocessing
import concurrent.futures
from collections import OrderedDict

import scipy
import scipy.stats
import numpy as np
import pandas as pd
import keras
from keras.applications.imagenet_utils import preprocess_input

NUM_WORKERS = int(round(multiprocessing.cpu_count() / 2))


class MyTimer():
    def __init__(self):
        self.start = time.time()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        end = time.time()
        runtime = end - self.start
        msg = '({time} seconds to complete)'
        print(msg.format(time=round(runtime, 5)))


class DupeDetector:
    def __init__(self):
        self.DUPE_DETECTION_MODELS = OrderedDict()
        self.DUPE_DETECTION_MODELS['VGG16'] = keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, pooling='avg')

        # # TODO: disabled for speeed while developing
        # self.DUPE_DETECTION_MODELS['Xception'] =          keras.applications.xception.Xception(weights='imagenet', include_top=False, pooling='avg')
        # self.DUPE_DETECTION_MODELS['InceptionResNetV2'] = keras.applications.inception_resnet_v2.InceptionResNetV2(weights='imagenet', include_top=False, pooling='avg')
        # self.DUPE_DETECTION_MODELS['DenseNet201'] =       keras.applications.DenseNet201(weights='imagenet', include_top=False, pooling='avg')
        # self.DUPE_DETECTION_MODELS['InceptionV3'] =       keras.applications.inception_v3.InceptionV3(weights='imagenet', include_top=False, pooling='avg')

        # # Unused models
        # self.DUPE_DETECTION_MODELS['VGG19'] =             keras.applications.vgg19.VGG19(weights='imagenet', include_top=False, pooling='avg')
        # self.DUPE_DETECTION_MODELS['MobileNet'] =         keras.applications.mobilenet.MobileNet(weights='imagenet', include_top=False, pooling='avg')
        # self.DUPE_DETECTION_MODELS['ResNet50'] =          keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False, pooling='avg')

    def __prepare_fingerprint_for_export(self, image_feature_data):
        image_feature_data_arr = np.char.mod('%f', image_feature_data)  # convert from Numpy to a list of values
        x_data = np.asarray(image_feature_data_arr).astype(
            'float64')  # convert image data to float64 matrix. float64 is need for bh_sne
        image_fingerprint_vector = x_data.reshape((x_data.shape[0], -1))
        return image_fingerprint_vector

    def compute_deep_learning_features(self, image):
        # the image is now in an array of shape (3, 224, 224) but we need to expand it to (1, 2, 224, 224) as
        # Keras is expecting a list of images
        x = np.expand_dims(image, axis=0)
        x = preprocess_input(x)

        fingerprints = {}
        for k, v in self.DUPE_DETECTION_MODELS.items():
            features = v.predict(x)[0]  # extract the features
            fingerprint_vector = self.__prepare_fingerprint_for_export(features)
            fingerprints[k] = fingerprint_vector

        return fingerprints


def hoeffd_inner_loop_func(i, R, S):
    # See slow_exact_hoeffdings_d_func for definition of R, S
    Q_i = 1 + sum(np.logical_and(R < R[i], S < S[i]))
    Q_i = Q_i + (1 / 4) * (sum(np.logical_and(R == R[i], S == S[i])) - 1)
    Q_i = Q_i + (1 / 2) * sum(np.logical_and(R == R[i], S < S[i]))
    Q_i = Q_i + (1 / 2) * sum(np.logical_and(R < R[i], S == S[i]))
    return Q_i


def generate_bootstrap_sample_func(original_length_of_input, sample_size):
    bootstrap_indices = np.array([random.randint(1, original_length_of_input) for x in range(sample_size)])
    return bootstrap_indices


def compute_average_and_stdev_of_25th_to_75th_percentile_func(input_vector):
    input_vector = np.array(input_vector)
    percentile_25 = np.percentile(input_vector, 25)
    percentile_75 = np.percentile(input_vector, 75)
    trimmed_vector = input_vector[input_vector > percentile_25]
    trimmed_vector = trimmed_vector[trimmed_vector < percentile_75]
    trimmed_vector_avg = np.mean(trimmed_vector)
    trimmed_vector_stdev = np.std(trimmed_vector)
    return trimmed_vector_avg, trimmed_vector_stdev


def compute_bootstrapped_hoeffdings_d_func(x, y, sample_size):
    x = np.array(x)
    y = np.array(y)
    assert (x.size == y.size)
    original_length_of_input = x.size
    bootstrap_sample_indices = generate_bootstrap_sample_func(original_length_of_input - 1, sample_size)
    N = sample_size
    x_bootstrap_sample = x[bootstrap_sample_indices]
    y_bootstrap_sample = y[bootstrap_sample_indices]
    R_bootstrap = scipy.stats.rankdata(x_bootstrap_sample)
    S_bootstrap = scipy.stats.rankdata(y_bootstrap_sample)
    hoeffdingd = functools.partial(hoeffd_inner_loop_func, R=R_bootstrap, S=S_bootstrap)
    Q_bootstrap = [hoeffdingd(x) for x in range(sample_size)]
    Q = np.array(Q_bootstrap)
    D1 = sum(((Q - 1) * (Q - 2)))
    D2 = sum((R_bootstrap - 1) * (R_bootstrap - 2) * (S_bootstrap - 1) * (S_bootstrap - 2))
    D3 = sum((R_bootstrap - 2) * (S_bootstrap - 2) * (Q - 1))
    D = 30 * ((N - 2) * (N - 3) * D1 + D2 - 2 * (N - 2) * D3) / (N * (N - 1) * (N - 2) * (N - 3) * (N - 4))
    return D


def apply_bootstrap_hoeffd_func(x, y, sample_size, ii):
    verbose = 0
    if verbose:
        print('Bootstrap ' + str(ii) + ' started...')
    return compute_bootstrapped_hoeffdings_d_func(x, y, sample_size)


def bootstrapped_hoeffd(x, y, sample_size, number_of_bootstraps):
    list_of_Ds = list()
    with concurrent.futures.ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor:
        inputs = range(number_of_bootstraps)
        f = functools.partial(apply_bootstrap_hoeffd_func, x, y, sample_size)
        for result in executor.map(f, inputs):
            list_of_Ds.append(result)

    robust_average_D, robust_stdev_D = compute_average_and_stdev_of_25th_to_75th_percentile_func(list_of_Ds)
    return robust_average_D


# def slow_exact_hoeffdings_d_func(x, y, pool):
#     # Based on code from here: https://stackoverflow.com/a/9322657/1006379
#     # For background see: https://projecteuclid.org/download/pdf_1/euclid.aoms/1177730150
#     x = np.array(x)
#     y = np.array(y)
#     N = x.shape[0]
#     print('Computing tied ranks...')
#     with MyTimer():
#         R = scipy.stats.rankdata(x, method='average')
#         S = scipy.stats.rankdata(y, method='average')
#     if 0:
#         print('Computing Q with list comprehension...')
#         with MyTimer():
#             Q = [hoeffd_inner_loop_func(i, R, S) for i in range(N)]
#     print('Computing Q with multiprocessing...')
#     with MyTimer():
#         hoeffd = functools.partial(hoeffd_inner_loop_func, R=R, S=S)
#         Q = pool.map(hoeffd, range(N))
#     print('Computing helper arrays...')
#     with MyTimer():
#         Q = np.array(Q)
#         D1 = sum(((Q - 1) * (Q - 2)))
#         D2 = sum((R - 1) * (R - 2) * (S - 1) * (S - 2))
#         D3 = sum((R - 2) * (S - 2) * (Q - 1))
#         D = 30 * ((N - 2) * (N - 3) * D1 + D2 - 2 * (N - 2) * D3) / (N * (N - 1) * (N - 2) * (N - 3) * (N - 4))
#     print('Exact Hoeffding D: ' + str(round(D, 8)))
#     return D


# def compute_parallel_bootstrapped_bagged_hoeffdings_d_func(x, y, sample_size, number_of_bootstraps, pool):
#     def apply_bootstrap_hoeffd_func(ii):
#         verbose = 0
#         if verbose:
#             print('Bootstrap ' + str(ii) + ' started...')
#         return compute_bootstrapped_hoeffdings_d_func(x, y, pool, sample_size)
#
#     list_of_Ds = list()
#     with concurrent.futures.ThreadPoolExecutor() as executor:
#         inputs = range(number_of_bootstraps)
#         for result in executor.map(apply_bootstrap_hoeffd_func, inputs):
#             list_of_Ds.append(result)
#     robust_average_D, robust_stdev_D = compute_average_and_stdev_of_25th_to_75th_percentile_func(list_of_Ds)
#     return list_of_Ds, robust_average_D, robust_stdev_D


def measure_similarity(candidate_image_fingerprint_transposed_values, final_combined_image_fingerprint_df):
    # For debugging: path_to_art_image_file = glob.glob(dupe_detection_test_images_base_folder_path+'*')[0]
    # spearman__dupe_threshold = 0.86
    # kendall__dupe_threshold = 0.80
    # hoeffding__dupe_threshold = 0.48
    spearman__dupe_threshold = 0.6
    kendall__dupe_threshold = 0.60
    hoeffding__dupe_threshold = 0.3
    strictness_factor = 0.99
    kendall_max = 0
    hoeffding_max = 0
    print('\nChecking if candidate image is a likely duplicate of a previously registered artwork:\n')
    print('Retrieving image fingerprints of previously registered images from local database...')
    with MyTimer():
        # final_combined_image_fingerprint_df = get_all_image_fingerprints_from_dupe_detection_database_as_dataframe_func()
        registered_image_fingerprints_transposed_values = final_combined_image_fingerprint_df.iloc[:, 2:].T.values
    number_of_previously_registered_images_to_compare = len(final_combined_image_fingerprint_df)
    length_of_each_image_fingerprint_vector = len(final_combined_image_fingerprint_df.columns)
    print('Comparing candidate image to the fingerprints of ' + str(
        number_of_previously_registered_images_to_compare) + ' previously registered images. Each fingerprint consists of ' + str(
        length_of_each_image_fingerprint_vector) + ' numbers.')
    print('Computing image fingerprint of candidate image...')
    # with MyTimer():
        # candidate_image_fingerprint = get_image_deep_learning_features_combined_vector_for_single_image_func(
        #     path_to_art_image_file)
        # candidate_image_fingerprint_transposed_values = candidate_image_fingerprint.iloc[:,
        #                                                 2:].T.values.flatten().tolist()
    print(
        "Computing Spearman's Rho, which is fast to compute (We only perform the slower tests on the fingerprints that have a high Rho).")
    with MyTimer():
        similarity_score_vector__spearman_all = []
        for i in range(number_of_previously_registered_images_to_compare):
            part = registered_image_fingerprints_transposed_values[:, i]
            correlation = scipy.stats.spearmanr(candidate_image_fingerprint_transposed_values, part).correlation
            similarity_score_vector__spearman_all.append(correlation)
        spearman_max = np.array(similarity_score_vector__spearman_all).max()
        indices_of_spearman_scores_above_threshold = \
        np.nonzero(np.array(similarity_score_vector__spearman_all) >= strictness_factor * spearman__dupe_threshold)[
            0].tolist()
        percentage_of_fingerprints_requiring_further_testing = len(indices_of_spearman_scores_above_threshold) / len(
            similarity_score_vector__spearman_all)
    print('Selected ' + str(
        len(indices_of_spearman_scores_above_threshold)) + ' fingerprints for further testing (' + str(
        round(100 * percentage_of_fingerprints_requiring_further_testing,
              2)) + '% of the total registered fingerprints).')
    list_of_fingerprints_requiring_further_testing = [
        registered_image_fingerprints_transposed_values[:, current_index].tolist() for current_index in
        indices_of_spearman_scores_above_threshold]
    similarity_score_vector__spearman = [
        scipy.stats.spearmanr(candidate_image_fingerprint_transposed_values, x).correlation for x in
        list_of_fingerprints_requiring_further_testing]
    assert (all(np.array(similarity_score_vector__spearman) >= strictness_factor * spearman__dupe_threshold))


    print("Now computing Kendall's Tao for selected fingerprints...")
    with MyTimer():
        if len(list_of_fingerprints_requiring_further_testing) > 0:
            similarity_score_vector__kendall = [
                scipy.stats.kendalltau(candidate_image_fingerprint_transposed_values, x).correlation for x in
                list_of_fingerprints_requiring_further_testing]
            kendall_max = np.array(similarity_score_vector__kendall).max()

            if kendall_max >= strictness_factor * kendall__dupe_threshold:
                indices_of_kendall_scores_above_threshold = list(np.nonzero(
                    np.array(similarity_score_vector__kendall) >= strictness_factor * kendall__dupe_threshold)[0])
                list_of_fingerprints_requiring_even_further_testing = [
                    list_of_fingerprints_requiring_further_testing[current_index] for current_index in
                    indices_of_kendall_scores_above_threshold]
            else:
                list_of_fingerprints_requiring_even_further_testing = []
                indices_of_kendall_scores_above_threshold = []
            percentage_of_fingerprints_requiring_further_testing = len(indices_of_kendall_scores_above_threshold) / len(
                similarity_score_vector__spearman_all)
        else:
            indices_of_kendall_scores_above_threshold = []

    if len(indices_of_kendall_scores_above_threshold) > 0:
        print('Selected ' + str(
            len(indices_of_kendall_scores_above_threshold)) + ' fingerprints for further testing (' + str(
            round(100 * percentage_of_fingerprints_requiring_further_testing,
                  2)) + '% of the total registered fingerprints).')
        print('Now computing bootstrapped Hoeffding D for selected fingerprints...')
        sample_size = 80
        number_of_bootstraps = 30
        with MyTimer():
            print('Sample Size: ' + str(sample_size) + '; Number of Bootstraps: ' + str(number_of_bootstraps))
            similarity_score_vector__hoeffding = []
            for current_fingerprint in list_of_fingerprints_requiring_even_further_testing:
                tmp = bootstrapped_hoeffd(candidate_image_fingerprint_transposed_values, current_fingerprint,
                                          sample_size, number_of_bootstraps)
                similarity_score_vector__hoeffding.append(tmp)

            hoeffding_max = np.array(similarity_score_vector__hoeffding).max()
            if hoeffding_max >= strictness_factor * hoeffding__dupe_threshold:
                indices_of_hoeffding_scores_above_threshold = list(np.nonzero(
                    np.array(similarity_score_vector__hoeffding) >= strictness_factor * hoeffding__dupe_threshold)[0])
                list_of_fingerprints_of_suspected_dupes = [
                    list_of_fingerprints_requiring_even_further_testing[current_index] for current_index in
                    indices_of_hoeffding_scores_above_threshold]
            else:
                list_of_fingerprints_of_suspected_dupes = []
                indices_of_hoeffding_scores_above_threshold = []
        if len(list_of_fingerprints_of_suspected_dupes) > 0:
            is_likely_dupe = 1
            print('\n\nWARNING! Art image file appears to be a duplicate!')
            print('Candidate Image appears to be a duplicate of the image fingerprint beginning with ' + str(
                list_of_fingerprints_of_suspected_dupes[0][0:5]))
            fingerprint_of_image_that_candidate_is_a_dupe_of = list_of_fingerprints_of_suspected_dupes[0]
            for ii in range(number_of_previously_registered_images_to_compare):
                current_fingerprint = registered_image_fingerprints_transposed_values[:, ii].tolist()
                if current_fingerprint == fingerprint_of_image_that_candidate_is_a_dupe_of:
                    sha_hash_of_most_similar_registered_image = final_combined_image_fingerprint_df.iloc[ii, 0]
            print(
                'The SHA256 hash of the registered artwork that is similar to the candidate image: ' + sha_hash_of_most_similar_registered_image)
        else:
            is_likely_dupe = 0
    else:
        is_likely_dupe = 0
    if not is_likely_dupe:
        print(
            '\n\nArt image file appears to be original! (i.e., not a duplicate of an existing image in the image fingerprint database)')
    # similarity_score_vector__rdc = [calculate_randomized_dependence_coefficient_func(np.array(candidate_image_fingerprint_transposed_values), np.array(x)) for x in list_of_fingerprints_requiring_further_testing]
    column_headers = ['spearman__dupe_threshold', 'kendall__dupe_threshold', 'hoeffding__dupe_threshold',
                      'strictness_factor', 'number_of_previously_registered_images_to_compare', 'spearman_max',
                      'kendall_max', 'hoeffding_max']
    params_df = pd.DataFrame(
        [spearman__dupe_threshold, kendall__dupe_threshold, hoeffding__dupe_threshold, strictness_factor,
         float(number_of_previously_registered_images_to_compare), spearman_max, kendall_max, hoeffding_max]).T
    params_df.columns = column_headers
    params_df = params_df.T
    return is_likely_dupe, params_df
