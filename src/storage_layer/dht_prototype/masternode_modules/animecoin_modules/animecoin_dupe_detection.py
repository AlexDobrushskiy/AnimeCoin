import time
from collections import OrderedDict

import scipy
import numpy as np
import keras
from keras.applications.imagenet_utils import preprocess_input


class MyTimer():
    def __init__(self):
        self.start = time.time()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        end = time.time()
        runtime = end - self.start
        msg = '({time} seconds to complete)'
        print(msg.format(time=round(runtime, 5)))


SPEARMAN_THRESHOLD = 0.86
KENDALL_THRESHOLD = 0.80
HOEFFDING_THREHOLD = 0.48
STRICTNESS_FACTOR = 0.99
KENDALL_MAX = 0
HOEFFDING_MAX = 0


class DupeDetector:
    def __init__(self):
        self.DUPE_DETECTION_MODELS = OrderedDict()
        self.DUPE_DETECTION_MODELS['VGG16'] = keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, pooling='avg')

        # # TODO: disabled for speeed while developing
        # self.DUPE_DETECTION_MODELS['Xception'] =          keras.applications.xception.Xception(weights='imagenet', include_top=False, pooling='avg')
        # self.DUPE_DETECTION_MODELS['InceptionResNetV2'] = keras.applications.inception_resnet_v2.InceptionResNetV2(weights='imagenet', include_top=False, pooling='avg')
        # self.DUPE_DETECTION_MODELS['DenseNet201'] =       keras.applications.DenseNet201(weights='imagenet', include_top=False, pooling='avg')
        # self.DUPE_DETECTION_MODELS['InceptionV3'] =       keras.applications.inception_v3.InceptionV3(weights='imagenet', include_top=False, pooling='avg')

        # # Unused models
        # self.DUPE_DETECTION_MODELS['VGG19'] =             keras.applications.vgg19.VGG19(weights='imagenet', include_top=False, pooling='avg')
        # self.DUPE_DETECTION_MODELS['MobileNet'] =         keras.applications.mobilenet.MobileNet(weights='imagenet', include_top=False, pooling='avg')
        # self.DUPE_DETECTION_MODELS['ResNet50'] =          keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False, pooling='avg')

    def __prepare_fingerprint_for_export(self, image_feature_data):
        image_feature_data_arr = np.char.mod('%f', image_feature_data)  # convert from Numpy to a list of values
        x_data = np.asarray(image_feature_data_arr).astype(
            'float64')  # convert image data to float64 matrix. float64 is need for bh_sne
        image_fingerprint_vector = x_data.reshape((x_data.shape[0], -1))
        return image_fingerprint_vector

    def compute_deep_learning_features(self, image):
        # the image is now in an array of shape (3, 224, 224) but we need to expand it to (1, 2, 224, 224) as
        # Keras is expecting a list of images
        x = np.expand_dims(image, axis=0)
        x = preprocess_input(x)

        fingerprints = {}
        for k, v in self.DUPE_DETECTION_MODELS.items():
            features = v.predict(x)[0]  # extract the features
            fingerprint_vector = self.__prepare_fingerprint_for_export(features)
            fingerprints[k] = fingerprint_vector

        return fingerprints


def measure_similarity(candidate_fingerprint, final_combined_image_fingerprint_df):
    registered_image_fingerprints_transposed_values = final_combined_image_fingerprint_df.iloc[:, 2:].T.values

    print('Comparing candidate image to the fingerprints of %s previously registered images.' % (len(final_combined_image_fingerprint_df)))
    print('Each fingerprint consists of %s' % (len(final_combined_image_fingerprint_df.columns)))

    print('Computing image fingerprint of candidate image...')
    with MyTimer():
        candidate_image_fingerprint_transposed_values = candidate_fingerprint.iloc[:, 2:].T.values.flatten().tolist()

    print(
        "Computing Spearman's Rho, which is fast to compute (We only perform the slower tests on the fingerprints that have a high Rho).")
    with MyTimer():
        similarity_score_vector__spearman_all = [scipy.stats.spearmanr(candidate_image_fingerprint_transposed_values,
                                                                       registered_image_fingerprints_transposed_values[
                                                                       :, ii]).correlation for ii in
                                                 range(len(final_combined_image_fingerprint_df))]
        spearman_max = np.array(similarity_score_vector__spearman_all).max()
        indices_of_spearman_scores_above_threshold = \
            np.nonzero(np.array(similarity_score_vector__spearman_all) >= STRICTNESS_FACTOR * SPEARMAN_THRESHOLD)[
                0].tolist()
        percentage_of_fingerprints_requiring_further_testing = len(indices_of_spearman_scores_above_threshold) / len(
            similarity_score_vector__spearman_all)
    print('Selected ' + str(
        len(indices_of_spearman_scores_above_threshold)) + ' fingerprints for further testing (' + str(
        round(100 * percentage_of_fingerprints_requiring_further_testing,
              2)) + '% of the total registered fingerprints).')
    list_of_fingerprints_requiring_further_testing = [
        registered_image_fingerprints_transposed_values[:, current_index].tolist() for current_index in
        indices_of_spearman_scores_above_threshold]
    similarity_score_vector__spearman = [
        scipy.stats.spearmanr(candidate_image_fingerprint_transposed_values, x).correlation for x in
        list_of_fingerprints_requiring_further_testing]
    assert (all(np.array(similarity_score_vector__spearman) >= STRICTNESS_FACTOR * SPEARMAN_THRESHOLD))


    print("Now computing Kendall's Tao for selected fingerprints...")
    with MyTimer():
        if len(list_of_fingerprints_requiring_further_testing) > 0:
            similarity_score_vector__kendall = [
                scipy.stats.kendalltau(candidate_image_fingerprint_transposed_values, x).correlation for x in
                list_of_fingerprints_requiring_further_testing]
            kendall_max = np.array(similarity_score_vector__kendall).max()

            if kendall_max >= STRICTNESS_FACTOR * KENDALL_THRESHOLD:
                indices_of_kendall_scores_above_threshold = list(np.nonzero(
                    np.array(similarity_score_vector__kendall) >= STRICTNESS_FACTOR * KENDALL_THRESHOLD)[0])
                list_of_fingerprints_requiring_even_further_testing = [
                    list_of_fingerprints_requiring_further_testing[current_index] for current_index in
                    indices_of_kendall_scores_above_threshold]
            else:
                list_of_fingerprints_requiring_even_further_testing = []
                indices_of_kendall_scores_above_threshold = []
            percentage_of_fingerprints_requiring_further_testing = len(indices_of_kendall_scores_above_threshold) / len(
                similarity_score_vector__spearman_all)
        else:
            indices_of_kendall_scores_above_threshold = []

    if len(indices_of_kendall_scores_above_threshold) > 0:
        print('Selected ' + str(
            len(indices_of_kendall_scores_above_threshold)) + ' fingerprints for further testing (' + str(
            round(100 * percentage_of_fingerprints_requiring_further_testing,
                  2)) + '% of the total registered fingerprints).')
        print('Now computing bootstrapped Hoeffding D for selected fingerprints...')
        sample_size = 80
        number_of_bootstraps = 30
        with MyTimer():
            print('Sample Size: ' + str(sample_size) + '; Number of Bootstraps: ' + str(number_of_bootstraps))
            similarity_score_vector__hoeffding = [
                bootstrapped_hoeffd(candidate_image_fingerprint_transposed_values, current_fingerprint, sample_size,
                                    number_of_bootstraps, pool) for current_fingerprint in
                list_of_fingerprints_requiring_even_further_testing]
            hoeffding_max = np.array(similarity_score_vector__hoeffding).max()
            if hoeffding_max >= STRICTNESS_FACTOR * HOEFFDING_THREHOLD:
                indices_of_hoeffding_scores_above_threshold = list(np.nonzero(
                    np.array(similarity_score_vector__hoeffding) >= STRICTNESS_FACTOR * HOEFFDING_THREHOLD)[0])
                list_of_fingerprints_of_suspected_dupes = [
                    list_of_fingerprints_requiring_even_further_testing[current_index] for current_index in
                    indices_of_hoeffding_scores_above_threshold]
            else:
                list_of_fingerprints_of_suspected_dupes = []
                indices_of_hoeffding_scores_above_threshold = []
        if len(list_of_fingerprints_of_suspected_dupes) > 0:
            is_likely_dupe = 1
            print('\n\nWARNING! Art image file appears to be a duplicate!')
            print('Candidate Image appears to be a duplicate of the image fingerprint beginning with ' + str(
                list_of_fingerprints_of_suspected_dupes[0][0:5]))
            fingerprint_of_image_that_candidate_is_a_dupe_of = list_of_fingerprints_of_suspected_dupes[0]
            for ii in range(len(final_combined_image_fingerprint_df)):
                current_fingerprint = registered_image_fingerprints_transposed_values[:, ii].tolist()
                if current_fingerprint == fingerprint_of_image_that_candidate_is_a_dupe_of:
                    sha_hash_of_most_similar_registered_image = final_combined_image_fingerprint_df.iloc[ii, 0]
            print(
                'The SHA256 hash of the registered artwork that is similar to the candidate image: ' + sha_hash_of_most_similar_registered_image)
        else:
            is_likely_dupe = 0
    else:
        is_likely_dupe = 0
    if not is_likely_dupe:
        print(
            '\n\nArt image file appears to be original! (i.e., not a duplicate of an existing image in the image fingerprint database)')
    # similarity_score_vector__rdc = [calculate_randomized_dependence_coefficient_func(np.array(candidate_image_fingerprint_transposed_values), np.array(x)) for x in list_of_fingerprints_requiring_further_testing]
    column_headers = ['spearman__dupe_threshold', 'kendall__dupe_threshold', 'hoeffding__dupe_threshold',
                      'strictness_factor', 'number_of_previously_registered_images_to_compare', 'spearman_max',
                      'kendall_max', 'hoeffding_max']
    params_df = pd.DataFrame(
        [SPEARMAN_THRESHOLD, KENDALL_THRESHOLD, HOEFFDING_THREHOLD, STRICTNESS_FACTOR,
         float(len(final_combined_image_fingerprint_df)), spearman_max, kendall_max, hoeffding_max]).T
    params_df.columns = column_headers
    params_df = params_df.T
    return is_likely_dupe, params_df
