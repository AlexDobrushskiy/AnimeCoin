import asyncio
import os
import logging
import random

import zmq
import zmq.asyncio

from .settings import NetWorkSettings
from .chunk_storage import ChunkStorage
from .masternode import MasterNodeManager
from .helpers import get_digest, get_hexdigest, hex_to_int, int_to_hex
from .animecoin_modules.animecoin_rpc import pack_and_sign, verify_and_unpack


class Chunk:
    def __init__(self, chunkid, exists=False, verified=False, is_ours=False):
        if type(chunkid) != int:
            raise ValueError("chunkid is not int!")

        self.chunkid = chunkid
        self.exists = exists
        self.verified = verified
        self.is_ours = is_ours

    def __str__(self):
        return "chunkid: %s, exists: %s, verified: %s, is_ours: %s" % (self.chunkid, self.exists,
                                                                       self.verified, self.is_ours)


class ChunkManager:
    def __init__(self, name, nodeid, masternode_settings, masternode_list):
        # node's name in logfiles
        self.__name = name

        # initialize logger
        # IMPORTANT: we must ALWAYS use self.__logger.* for logging and not logging.*,
        # since we need instance-level logging
        self.__logger = None
        self.__initlogging()

        # our node id
        self.__nodeid = hex_to_int(nodeid)

        # settings
        self.__masternode_settings = masternode_settings

        # the actual storage layer
        self.__storagedir = os.path.join(self.__masternode_settings.BASEDIR, "chunkdata")
        self.__storage = ChunkStorage(self.__storagedir, mode=0o0700)

        # databases we keep
        self.__alias_db = {}
        self.__file_db = {}

        # helper lookup table for alias generation and other nodes
        self.__alias_digests = []

        # table of every chunk we know of
        self.__chunk_table = set()

        # masternode manager
        self.__mn_manager = MasterNodeManager(masternode_list)

        # todolist
        self.__todolist = asyncio.Queue()

        # message queue
        mn = self.__mn_manager.get(self.__nodeid)
        self.__zmq = zmq.asyncio.Context().socket(zmq.ROUTER)
        self.__zmq.setsockopt(zmq.IDENTITY, bytes(str(self.__nodeid), "utf-8"))
        self.__zmq.bind("tcp://%s:%s" % (mn.ip, mn.port))

        # run other initializations
        self.__initialize()

    def __str__(self):
        return "%s" % self.__nodeid

    def __initialize(self):
        self.__logger.debug("Initializing")

        # sanity check
        self.__mn_manager.get(self.__nodeid)

        # initializations
        self.__init_alias_digests()
        self.__discover_files_in_local_storage()
        self.__recalculate_ownership_of_all_chunks()
        self.__purge_orphaned_files()
        self.__purge_orphaned_db_entries()
        self.__dump_internal_stats()

    def __initlogging(self):
        self.__logger = logging.getLogger(self.__name)
        self.__logger.setLevel(logging.DEBUG)

        formatter = logging.Formatter(' %(asctime)s - ' + self.__name + ' - %(levelname)s - %(message)s')
        consolehandler = logging.StreamHandler()
        consolehandler.setFormatter(formatter)
        self.__logger.addHandler(consolehandler)

    def __init_alias_digests(self):
        for i in range(NetWorkSettings.REPLICATION_FACTOR):
            digest = get_digest(i.to_bytes(1, byteorder='big') + NetWorkSettings.ALIAS_SEED)
            digest_int = int.from_bytes(digest, byteorder='big')
            self.__logger.debug("Alias digest %s -> %s" % (i, digest_int))
            self.__alias_digests.append(digest_int)

    def __discover_files_in_local_storage(self):
        self.__logger.debug("Indexing local files in %s" % self.__storagedir)

        # reads the filesystem and fills our DB of chunks we have
        for chunkid in self.__storage.index():
            # verify the chunk
            if not self.__storage.verify(chunkid):
                self.__logger.warning("Verify failed for chunkid %s, deleting" % chunkid)
                self.__storage.delete(chunkid)
                continue

            # update our database
            if self.__file_db.get(chunkid) is None:
                chunk = Chunk(chunkid=chunkid)
                self.__file_db[chunkid] = chunk
            else:
                chunk = self.__file_db[chunkid]

            chunk.exists = True
            chunk.verified = True
            chunk.is_ours = False  # we don't know yet whether this is our chunk

        self.__logger.debug("Discovered %s files in local storage" % len(self.__file_db))

    def __recalculate_ownership_of_all_chunks(self):
        for chunkid in self.__chunk_table:
            self.__update_chunk_ownership(chunkid)

    def __generate_aliases(self, chunkid):
        for alias_digest in self.__alias_digests:
            yield alias_digest ^ chunkid

    def __update_chunk_ownership(self, chunkid):
        actual_chunk_is_ours = False

        # maintain the alias table
        alias_updates = []
        for alt_key in self.__generate_aliases(chunkid):
            alias_owned = self.__we_own_this_alt_key(alt_key)
            alias_updates.append((alt_key, alias_owned))

            if alias_owned:
                # if we own the alias we own the chunk
                actual_chunk_is_ours = True

        # maintain alias db
        for alt_key, alt_key_owned in alias_updates:
            if alt_key_owned:
                # this alias points to us
                # self.__logger.debug("Alt key %s is now OWNED (chunkid: %s)" % (alt_key, chunkid))
                self.__alias_db[alt_key] = chunkid
            else:
                # this alias not longer points to us
                # self.__logger.debug("Alt key %s is now DISOWNED (chunkid: %s)" % (alt_key, chunkid))
                if self.__alias_db.get(alt_key) is not None:
                    del self.__alias_db[alt_key]

        # if even a single alias says we own this chunk, we do
        if actual_chunk_is_ours:
            # maintain file db
            chunk = self.__get_or_create_chunk(chunkid)
            if not chunk.is_ours:
                # self.__logger.debug("Chunk %s is now OWNED" % chunkid)
                chunk.is_ours = True

            # if we don't have it or it's not verified for some reason, fetch it
            if not chunk.exists or not chunk.verified:
                # self.__logger.info("Chunk %s missing, added to todolist" % chunkid)
                self.__todolist.put_nowait(("MISSING_CHUNK", chunkid))
        else:
            # maintain file db
            chunk = self.__get_or_create_chunk(chunkid, create=False)
            if chunk is not None:
                if chunk.is_ours:
                    # self.__logger.debug("Chunk %s is now DISOWNED" % chunkid)
                    chunk.is_ours = False

        return actual_chunk_is_ours

    def __find_owners_for_chunk(self, chunkid):
        owners = set()
        for alt_key in self.__generate_aliases(chunkid):
            # found owner for this alt_key
            owner, min_distance = None, None
            for mn in self.__mn_manager.get_all():
                distance = alt_key ^ mn.nodeid
                if owner is None or distance < min_distance:
                    owner = mn.nodeid
                    min_distance = distance
            owners.add(owner)
        return owners

    def __we_own_this_alt_key(self, alt_key):
        my_distance = alt_key ^ self.__nodeid

        # check if we are the closest to this chunk
        store = True
        for othernodeid in self.__mn_manager.get_other_nodes(self.__nodeid):
            if alt_key ^ othernodeid < my_distance:
                store = False
                break

        return store

    def __purge_orphaned_files(self):
        self.__logger.info("Purging orphaned files")
        for chunkid, chunk in self.__file_db.items():
            if chunk.exists:
                if not chunk.is_ours or not chunk.verified:
                    self.__storage.delete(chunkid)

    def __purge_orphaned_db_entries(self):
        self.__logger.info("Purging orphaned DB entries")
        to_delete = []
        for chunkid, chunk in self.__file_db.items():
            if not chunk.exists and not chunk.is_ours:
                to_delete.append(chunkid)

        for chunkid in to_delete:
            del self.__file_db[chunkid]

    def __get_or_create_chunk(self, chunkid, create=True):
        if self.__file_db.get(chunkid) is None:
            if not create:
                return None
            else:
                self.__file_db[chunkid] = Chunk(chunkid=chunkid)

        chunk = self.__file_db[chunkid]
        return chunk

    def update_mn_list(self, masternode_list):
        added, removed = self.__mn_manager.update_maternode_list(masternode_list)
        if len(added) + len(removed) > 0:
            if self.__nodeid in removed:
                self.__logger.warning("I am removed from the MN list, aborting %s" % self.__nodeid)
                # return

            self.__logger.info("MN list has changed -> added: %s, removed: %s" % (added, removed))
            self.__dump_internal_stats("DB STAT Before")
            self.__recalculate_ownership_of_all_chunks()
            self.__purge_orphaned_files()
            self.__purge_orphaned_db_entries()
            self.__dump_internal_stats("DB STAT After")

    def load_full_chunks(self, chunks):
        # helper function to load chunks with data
        # this is for testing, since we have to bootstrap the system somehow
        self.__dump_internal_stats("DB STAT Before")
        for chunk_str, data in chunks:
            chunkid = int(chunk_str, 16)
            self.__chunk_table.add(chunkid)
            ours = self.__update_chunk_ownership(chunkid)
            if ours:
                self.__logger.debug("Chunk %s is loaded" % chunkid)
                self.__storage.put(chunkid, data)
        self.__dump_internal_stats("DB STAT After")

    def load_chunks(self, chunks):
        # helper function to load chunks without data
        self.__dump_internal_stats("DB STAT Before")
        for chunk_str in chunks:
            chunkid = int(chunk_str, 16)
            self.__chunk_table.add(chunkid)
            self.__update_chunk_ownership(chunkid)
        self.__dump_internal_stats("DB STAT After")

    def new_chunks_added_to_blockchain(self, chunks):
        self.__dump_internal_stats("DB STAT Before")
        for chunk_str in chunks:
            chunkid = int(chunk_str, 16)
            self.__chunk_table.add(chunkid)
            self.__update_chunk_ownership(chunkid)
        self.__dump_internal_stats("DB STAT After")

    def get_chunk_ownership(self, chunk_str):
        chunkid = int(chunk_str, 16)
        if self.__file_db.get(chunkid) is not None:
            return self.__file_db[chunkid].is_ours
        else:
            return False

    async def __send_rpc_spotcheck(self, mn, chunkid, start, end):
        self.__logger.debug("SPOTCHECK REQUEST to %s, chunkid: %s" % (int_to_hex(mn.nodeid), int_to_hex(chunkid)))

        # chunkid is bignum so we need to serialize it
        chunkid_str = int_to_hex(chunkid)
        request_msg = ["SPOTCHECK_REQ", {"chunkid": chunkid_str, "start": start, "end": end}]

        request_packet = pack_and_sign(self.__masternode_settings.PRIVKEY,
                                       self.__masternode_settings.PUBKEY,
                                       mn.pubkey, request_msg)

        response_packet = await mn.send_rpc_and_wait_for_response(request_packet)

        sender_id, response_msg = verify_and_unpack(response_packet, self.__masternode_settings.PUBKEY)

        rpcname, response_data = response_msg

        if rpcname != "SPOTCHECK_RESP":
            raise ValueError("Spotcheck response has rpc name: %s" % rpcname)

        if set(response_data.keys()) != {"digest"}:
            raise ValueError("RPC parameters are wrong for SPOTCHECK_RESP: %s" % response_data.keys())

        if type(response_data["digest"]) != str:
            raise TypeError("digest is not str: %s" % type(response_data["digest"]))

        response_digest = response_data["digest"]

        self.__logger.debug("SPOTCHECK RESPONSE from %s, msg: %s" % (int_to_hex(mn.nodeid), response_digest))

        return response_digest

    def __receive_rpc_spotcheck(self, sender_id, data):
        # NOTE: data is untrusted!
        if not isinstance(data, dict):
            raise TypeError("Data must be a dict!")

        if set(data.keys()) != {"chunkid", "start", "end"}:
            raise ValueError("Invalid arguments for spotcheck: %s" % (data.keys()))

        for k, v in data.items():
            if k in ["start", "end"]:
                if not isinstance(v, int):
                    raise TypeError("Invalid type for key %s in spotcheck" % k)
            else:
                if not isinstance(v, str):
                    raise TypeError("Invalid type for key %s in spotcheck" % k)

        chunkid = hex_to_int(data["chunkid"])
        start = data["start"]
        end = data["end"]

        # check if this is an actual chunk
        if chunkid not in self.__chunk_table:
            raise ValueError("This chunk is not in the chunk table: %s" % chunkid)

        # check if we should have this chunk
        if self.__nodeid not in self.__find_owners_for_chunk(chunkid):
            raise ValueError("chunk %s does not belong ot us!" % chunkid)

        # check if start and end are within parameters
        if start < 0:
            raise ValueError("start is < 0")
        if start >= end:
            raise ValueError("start >= end")
        if start > NetWorkSettings.CHUNKSIZE or end > NetWorkSettings.CHUNKSIZE:
            raise ValueError("start > CHUNKSIZE or end > CHUNKSIZE")

        # check if we have this chunk
        chunk = self.__file_db.get(chunkid)
        if chunk is None:
            pass
            # TODO: signal error, we don't have the chunk

        # verify the chunk
        # if not self.__storage.verify(chunkid):
        #     self.__logger.warning("Failed spotcheck for chunk %s" % chunkid)
        #     # TODO: signal error

        # generate digest
        data = self.__storage.get(chunkid, offset=start, length=end-start)
        digest = get_hexdigest(data)

        # generate response
        request_msg = ["SPOTCHECK_RESP", {"digest": digest}]

        response_packet = pack_and_sign(self.__masternode_settings.PRIVKEY,
                                        self.__masternode_settings.PUBKEY,
                                        sender_id, request_msg)

        return response_packet

    async def __send_rpc_fetchchunk(self, mn, chunkid):
        self.__logger.debug("FETCHCHUNK REQUEST to %s, chunkid: %s" % (int_to_hex(mn.nodeid), int_to_hex(chunkid)))

        # chunkid is bignum so we need to serialize it
        chunkid_str = int_to_hex(chunkid)
        request_msg = ["FETCHCHUNK_REQ", {"chunkid": chunkid_str}]

        request_packet = pack_and_sign(self.__masternode_settings.PRIVKEY,
                                       self.__masternode_settings.PUBKEY,
                                       mn.pubkey, request_msg)

        response_packet = await mn.send_rpc_and_wait_for_response(request_packet)

        sender_id, response_msg = verify_and_unpack(response_packet, self.__masternode_settings.PUBKEY)

        rpcname, response_data = response_msg

        if rpcname != "FETCHCHUNK_RESP":
            raise ValueError("FETCHCHUNK response has rpc name: %s" % rpcname)

        if set(response_data.keys()) != {"chunk"}:
            raise ValueError("RPC parameters are wrong for SPOTCHECK_RESP: %s" % response_data.keys())

        if type(response_data["chunk"]) != bytes:
            raise TypeError("chunk is not bytes: %s" % type(response_data["chunk"]))

        chunk = response_data["chunk"]

        # validate chunk
        digest = get_hexdigest(chunk)
        if digest != chunkid_str:
            raise ValueError("Got chunk data that does not match the digest!")

        return chunk

    def __receive_rpc_fetchchunk(self, sender_id, data):
        # NOTE: data is untrusted!
        if not isinstance(data, dict):
            raise TypeError("Data must be a dict!")

        if set(data.keys()) != {"chunkid"}:
            raise ValueError("Invalid arguments for spotcheck: %s" % (data.keys()))

        if not isinstance(data["chunkid"], str):
            raise TypeError("Invalid type for key chunkid in spotcheck")

        chunkid = hex_to_int(data["chunkid"])

        # check if this is an actual chunk
        if chunkid not in self.__chunk_table:
            raise ValueError("This chunk is not in the chunk table: %s" % chunkid)

        # check if we should have this chunk
        if self.__nodeid not in self.__find_owners_for_chunk(chunkid):
            raise ValueError("chunk %s does not belong ot us!" % chunkid)

        # check if we have this chunk
        chunk = self.__file_db.get(chunkid)
        if chunk is None:
            pass
            # TODO: signal error, we don't have the chunk

        # get data for chunk
        data = self.__storage.get(chunkid)

        # verify the chunk
        # if not self.__storage.verify(chunkid):
        #     self.__logger.warning("Failed spotcheck for chunk %s" % chunkid)
        #     # TODO: signal error

        # generate response
        request_msg = ["FETCHCHUNK_RESP", {"chunk": data}]

        response_packet = pack_and_sign(self.__masternode_settings.PRIVKEY,
                                        self.__masternode_settings.PUBKEY,
                                        sender_id, request_msg)

        return response_packet

    async def __zmq_process(self, ident, msg):
        sender_id, received_msg = verify_and_unpack(msg, self.__masternode_settings.PUBKEY)
        rpcname, data = received_msg

        if rpcname == "SPOTCHECK_REQ":
            reply_packet = self.__receive_rpc_spotcheck(sender_id, data)
        elif rpcname == "FETCHCHUNK_REQ":
            reply_packet = self.__receive_rpc_fetchchunk(sender_id, data)
        else:
            # TODO: signal error
            raise NotImplementedError("Invalid rpc name")

        await self.__zmq.send_multipart([ident, reply_packet])

    async def zmq_run_forever(self):
        while True:
            ident, msg = await self.__zmq.recv_multipart()  # waits for msg to be ready
            asyncio.ensure_future(self.__zmq_process(ident, msg))

    async def run_workers_forever(self):
        while True:
            self.__logger.debug("TODOLIST: queue size: %s" % self.__todolist.qsize())
            todoitem = await self.__todolist.get()

            itemtype, itemdata = todoitem
            if itemtype == "MISSING_CHUNK":
                chunkid = itemdata
                self.__logger.debug("Fetching chunk %s" % chunkid)

                # TODO: figure out who has this chunk, try them in order
                for owner in self.__find_owners_for_chunk(chunkid):
                    if owner == self.__nodeid:
                        continue
                    mn = self.__mn_manager.get(owner)

                    chunk = await self.__send_rpc_fetchchunk(mn, chunkid)
                    self.__logger.debug("Fetched chunk %s" % len(chunk))
                    break
            else:
                raise ValueError("Invalid todo type: %s" % itemtype)
            self.__todolist.task_done()

    async def issue_random_tests_forever(self, waittime):
        while True:
            await asyncio.sleep(waittime)

            chunks = random.sample(self.__file_db.keys(), 1)
            for chunkid in chunks:
                self.__logger.debug("Selected chunk %s for random check" % int_to_hex(chunkid))

                # pick a file we have for sure
                chunk = self.__file_db[chunkid]
                if not self.__storage.verify(chunkid):
                    self.__logger.warning("Self-initiated spotcheck failed on us for chunk: %s" % chunkid)
                    # TODO: We should resync the chunk and increment some counter
                    continue

                # TODO: do not read the whole file!
                data = self.__storage.get(chunk.chunkid)

                # pick a random range
                assert len(data) > 1024
                start = random.randint(0, len(data)-1024)
                end = start + 1024

                # calculate digest
                digest = get_hexdigest(data[start:end])
                self.__logger.debug("Digest for range %s - %s is: %s" % (start, end, digest))

                # find owners for all the alt keys who are not us
                owners = self.__find_owners_for_chunk(chunkid)
                if self.__nodeid in owners:
                    # we have already tested ourselves with verify()
                    owners.remove(self.__nodeid)

                # call RPC on all other MNs
                for owner in owners:
                    mn = self.__mn_manager.get(owner)

                    response_digest = await self.__send_rpc_spotcheck(mn, chunkid, start, end)

                    if response_digest != digest:
                        self.__logger.warning("SPOTCHECK FAILED for node %s (%s != %s)" % (owner, digest,
                                                                                           response_digest))
                    else:
                        self.__logger.debug("SPOTCHECK SUCCESS for node %s for chunk: %s" % (owner, digest))

                    # TODO: track successes/errors

    # DEBUG FUNCTIONS
    def __dump_internal_stats(self, msg=""):
        self.__logger.debug("%s -> Aliases: %s, file_db: %s, chunk_table: %s" % (msg, len(self.__alias_db),
                                                                    len(self.__file_db),
                                                                    len(self.__chunk_table)))

    def __dump_chunks(self):
        for k, v in self.__alias_db.items():
            self.__logger.debug("ALIAS %s: %s" % (k, v))
        for k, v in self.__file_db.items():
            self.__logger.debug("FILE %s: %s" % (k, v))
    # END
